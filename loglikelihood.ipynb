{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf42ee36-fba6-4c07-98a2-5dd0982b7f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# 读取训练文本数据，每行代表一个文本数据\n",
    "train_data = pd.read_csv('./corpus_0.csv', sep='\\t')\n",
    "train_data.fillna('', inplace=True)\n",
    "train_data_0 = train_data.iloc[1:]\n",
    "\n",
    "# 定义分词函数\n",
    "def cut_words(sentence):\n",
    "    return [word for word in jieba.cut(sentence) if word.strip()]\n",
    "\n",
    "# 预处理语料库\n",
    "sentences_1 = [cut_words(str(text)) for text in train_data_0['corpus']]\n",
    "\n",
    "sentences_as_texts_1 = [' '.join(sentence) for sentence in sentences]\n",
    "\n",
    "# 读取训练文本数据，每行代表一个文本数据\n",
    "train_data = pd.read_csv('./corpus_5.csv', sep='\\t')\n",
    "train_data.fillna('', inplace=True)\n",
    "train_data_5 = train_data.iloc[1:]\n",
    "\n",
    "# 定义分词函数\n",
    "def cut_words(sentence):\n",
    "    return [word for word in jieba.cut(sentence) if word.strip()]\n",
    "\n",
    "# 预处理语料库\n",
    "sentences_2 = [cut_words(str(text)) for text in train_data_5['corpus']]\n",
    "sentences_as_texts_2 = [' '.join(sentence) for sentence in sentences]\n",
    "\n",
    "\n",
    "# 构建CountVectorizer对象，并进行向量化\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "ref_vecs = vectorizer.fit_transform(sentences_as_texts_1)\n",
    "dom_vecs = vectorizer.fit_transform(sentences_as_texts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f562aef8-31b1-45c2-99d6-0e9e49abf849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "tf_words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ab397be7-5707-43f6-9ab8-91d3b0fd296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_likelihood(dom_vecs, ref_vecs, tf_words, n_words):  \n",
    "    \n",
    "    # get word occurances in domain corpus\n",
    "    O_dom = np.array(dom_vecs.sum(axis = 0).tolist()[0])\n",
    "    O_dom = O_dom.astype(float)\n",
    "    O_dom[O_dom == 0] = 0.00001  # avoid division by 0\n",
    "    \n",
    "    # get word occurances in ref corpus\n",
    "    O_ref = np.array(ref_vecs.sum(axis = 0).tolist()[0])\n",
    "    O_ref = O_ref.astype(float)\n",
    "    O_ref[O_ref == 0] = 0.00001  # avoid division by 0\n",
    "    \n",
    "    # get average observed frequencies\n",
    "    total_corpora_size = dom_vecs.shape[0]+ref_vecs.shape[0]\n",
    "    aved_freqs = (O_dom+O_ref)/total_corpora_size\n",
    "    \n",
    "    # get expected frequencies\n",
    "    E_dom = dom_vecs.shape[0]*aved_freqs\n",
    "    E_ref = ref_vecs.shape[0]*aved_freqs\n",
    "    \n",
    "    # compute log likelihoods\n",
    "    LL_ref = O_ref*np.log(O_ref/E_ref)\n",
    "    LL_dom = O_dom*np.log(O_dom/E_dom)\n",
    "    LL = 2*(LL_dom + LL_ref)\n",
    "    \n",
    "    # sort domain terms \n",
    "    sorted_dom_words = [tf_words[i] for i in LL.argsort()[::-1]]\n",
    "    \n",
    "    i = 0\n",
    "    top_dom_words = []\n",
    "    \n",
    "    while len(top_dom_words) < n_words:\n",
    "        \n",
    "        word = sorted_dom_words[i]\n",
    "        idx = tf_words.index(word)\n",
    "        dom_ratio = O_dom[idx]/dom_vecs.shape[0]\n",
    "        ref_ratio = O_ref[idx]/ref_vecs.shape[0]\n",
    "    \n",
    "        if dom_ratio > ref_ratio:\n",
    "            LL_score = LL[idx]\n",
    "            top_dom_words.append((word,round(LL_score,2)))\n",
    "            \n",
    "        i+=1\n",
    "        \n",
    "    return top_dom_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9a4a0a26-2700-4a81-86c8-e7d12166fb67",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14571/2417173319.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdom_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdom_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_14571/986725189.py\u001b[0m in \u001b[0;36mget_log_likelihood\u001b[0;34m(dom_vecs, ref_vecs, tf_words, n_words)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted_dom_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mdom_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mO_dom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdom_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mref_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mO_ref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mref_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_log_likelihood(dom_vecs, ref_vecs, tf_words, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4658ec0a-8ed6-4bc6-ae99-826ceea309fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
